{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 — Setup & First Iceberg Table\n",
    "\n",
    "In this notebook we will:\n",
    "1. Initialize a SparkSession with Iceberg support\n",
    "2. Create a namespace (database)\n",
    "3. Create our first Iceberg table\n",
    "4. Insert sample data\n",
    "5. Run basic queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark with Iceberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/23 13:51:16 WARN Utils: Your hostname, barkha-xg1 resolves to a loopback address: 127.0.1.1; using 192.168.1.227 instead (on interface enp195s0)\n",
      "26/02/23 13:51:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/barkha/.ivy2/cache\n",
      "The jars for the packages stored in: /home/barkha/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-b6070ec0-ae11-4c94-9584-2e7e1dcbc37f;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/barkha/iceberg-demo/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.7.1/iceberg-spark-runtime-3.5_2.12-1.7.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1!iceberg-spark-runtime-3.5_2.12.jar (863ms)\n",
      ":: resolution report :: resolve 463ms :: artifacts dl 865ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   1   |   1   |   0   ||   1   |   1   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-b6070ec0-ae11-4c94-9584-2e7e1dcbc37f\n",
      "\tconfs: [default]\n",
      "\t1 artifacts copied, 0 already retrieved (41794kB/13ms)\n",
      "26/02/23 13:51:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.3\n",
      "Iceberg catalog 'demo' is ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/23 13:51:28 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IcebergDemo\")\n",
    "    .master(\"local[*]\")\n",
    "    # Pull in the Iceberg runtime JAR (downloaded automatically)\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\")\n",
    "    # Register the Iceberg catalog\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.demo.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"../warehouse\")\n",
    "    # Iceberg SQL extensions (MERGE INTO, time-travel, etc.)\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(\"Iceberg catalog 'demo' is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Namespace\n",
    "\n",
    "A namespace in Iceberg is like a database — it groups related tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|ecommerce|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS demo.ecommerce\")\n",
    "spark.sql(\"SHOW NAMESPACES IN demo\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create an Iceberg Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created!\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|ecommerce|   orders|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS demo.ecommerce.orders (\n",
    "        order_id   INT,\n",
    "        customer   STRING,\n",
    "        product    STRING,\n",
    "        quantity   INT,\n",
    "        price      DOUBLE,\n",
    "        order_date DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "print(\"Table created!\")\n",
    "spark.sql(\"SHOW TABLES IN demo.ecommerce\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Insert Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 rows inserted.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO demo.ecommerce.orders VALUES\n",
    "        (1,  'Alice',   'Laptop',     1, 999.99,  DATE '2024-01-15'),\n",
    "        (2,  'Bob',     'Mouse',      2, 29.99,   DATE '2024-01-16'),\n",
    "        (3,  'Charlie', 'Keyboard',   1, 79.99,   DATE '2024-01-16'),\n",
    "        (4,  'Alice',   'Monitor',    1, 349.99,  DATE '2024-01-17'),\n",
    "        (5,  'Diana',   'Headphones', 3, 59.99,   DATE '2024-01-18')\n",
    "\"\"\")\n",
    "\n",
    "print(\"5 rows inserted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------+------+----------+\n",
      "|order_id|customer|   product|quantity| price|order_date|\n",
      "+--------+--------+----------+--------+------+----------+\n",
      "|       1|   Alice|    Laptop|       1|999.99|2024-01-15|\n",
      "|       2|     Bob|     Mouse|       2| 29.99|2024-01-16|\n",
      "|       3| Charlie|  Keyboard|       1| 79.99|2024-01-16|\n",
      "|       4|   Alice|   Monitor|       1|349.99|2024-01-17|\n",
      "|       5|   Diana|Headphones|       3| 59.99|2024-01-18|\n",
      "+--------+--------+----------+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------+\n",
      "|customer|num_orders|total_spent|\n",
      "+--------+----------+-----------+\n",
      "|   Alice|         2|    1349.98|\n",
      "|   Diana|         1|     179.97|\n",
      "| Charlie|         1|      79.99|\n",
      "|     Bob|         1|      59.98|\n",
      "+--------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quick aggregation\n",
    "spark.sql(\"\"\"\n",
    "    SELECT customer, COUNT(*) AS num_orders, ROUND(SUM(price * quantity), 2) AS total_spent\n",
    "    FROM demo.ecommerce.orders\n",
    "    GROUP BY customer\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Just Happened?\n",
    "\n",
    "Under the hood, Iceberg created:\n",
    "- **Data files** (Parquet) in `warehouse/ecommerce/orders/data/`\n",
    "- **Metadata files** (JSON + Avro) in `warehouse/ecommerce/orders/metadata/`\n",
    "\n",
    "This metadata layer is what gives Iceberg its superpowers — time travel, schema evolution, and more.\n",
    "\n",
    "**Next up:** CRUD operations in notebook 02!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
