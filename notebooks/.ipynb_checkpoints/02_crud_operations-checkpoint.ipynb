{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 — CRUD Operations\n",
    "\n",
    "Iceberg supports true row-level updates — unlike Hive tables where you'd have to rewrite entire partitions.\n",
    "\n",
    "In this notebook:\n",
    "1. INSERT more rows\n",
    "2. UPDATE existing rows\n",
    "3. DELETE rows\n",
    "4. MERGE INTO (upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IcebergDemo\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\")\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.demo.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"../warehouse\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"Spark + Iceberg ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current state of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INSERT — Add More Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO demo.ecommerce.orders VALUES\n",
    "        (6,  'Eve',   'Webcam',   1, 89.99,  DATE '2024-01-19'),\n",
    "        (7,  'Bob',   'USB Hub',  2, 24.99,  DATE '2024-01-20'),\n",
    "        (8,  'Charlie', 'Laptop', 1, 1099.99, DATE '2024-02-01')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. UPDATE — Fix a Price\n",
    "\n",
    "Oops — Alice's laptop was supposed to be $899.99, not $999.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    UPDATE demo.ecommerce.orders\n",
    "    SET price = 899.99\n",
    "    WHERE order_id = 1\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders WHERE order_id = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DELETE — Remove a Cancelled Order\n",
    "\n",
    "Eve cancelled her webcam order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    DELETE FROM demo.ecommerce.orders\n",
    "    WHERE order_id = 6\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MERGE INTO — Upsert New Data\n",
    "\n",
    "`MERGE INTO` lets you do an \"upsert\": update rows that match, insert rows that don't.\n",
    "\n",
    "This is one of Iceberg's most powerful features for incremental data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary view with incoming data\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW incoming_orders AS\n",
    "    SELECT * FROM VALUES\n",
    "        (2,  'Bob',   'Mouse',      2,  24.99,  DATE '2024-01-16'),  -- updated price\n",
    "        (9,  'Diana', 'Mouse',      1,  29.99,  DATE '2024-02-02'),  -- new order\n",
    "        (10, 'Frank', 'Keyboard',   2,  79.99,  DATE '2024-02-03')   -- new order\n",
    "    AS t(order_id, customer, product, quantity, price, order_date)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO demo.ecommerce.orders target\n",
    "    USING incoming_orders source\n",
    "    ON target.order_id = source.order_id\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaway\n",
    "\n",
    "| Operation  | Traditional Hive           | Iceberg                    |\n",
    "|------------|----------------------------|----------------------------|\n",
    "| INSERT     | Supported                  | Supported                  |\n",
    "| UPDATE     | Rewrite entire partition   | Row-level update           |\n",
    "| DELETE     | Rewrite entire partition   | Row-level delete           |\n",
    "| MERGE INTO | Not natively supported     | Built-in upsert support    |\n",
    "\n",
    "Every operation we just ran created a **new snapshot** — we'll explore those in the next notebook!\n",
    "\n",
    "**Next up:** Time travel in notebook 03!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
