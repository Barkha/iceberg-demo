{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 — CRUD Operations\n",
    "\n",
    "Iceberg supports true row-level updates — unlike Hive tables where you'd have to rewrite entire partitions.\n",
    "\n",
    "In this notebook:\n",
    "1. INSERT more rows\n",
    "2. UPDATE existing rows\n",
    "3. DELETE rows\n",
    "4. MERGE INTO (upsert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/23 13:54:23 WARN Utils: Your hostname, barkha-xg1 resolves to a loopback address: 127.0.1.1; using 192.168.1.227 instead (on interface enp195s0)\n",
      "26/02/23 13:54:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/barkha/.ivy2/cache\n",
      "The jars for the packages stored in: /home/barkha/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-cc68a922-4d0f-455f-88d1-6edb18fe8ae0;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 in central\n",
      ":: resolution report :: resolve 52ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-cc68a922-4d0f-455f-88d1-6edb18fe8ae0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/barkha/iceberg-demo/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0 artifacts copied, 1 already retrieved (0kB/2ms)\n",
      "26/02/23 13:54:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/23 13:54:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Iceberg ready.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IcebergDemo\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\")\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.demo.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"../warehouse\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"Spark + Iceberg ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current state of the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/23 13:54:42 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------+------+----------+\n",
      "|order_id|customer|   product|quantity| price|order_date|\n",
      "+--------+--------+----------+--------+------+----------+\n",
      "|       1|   Alice|    Laptop|       1|999.99|2024-01-15|\n",
      "|       2|     Bob|     Mouse|       2| 29.99|2024-01-16|\n",
      "|       3| Charlie|  Keyboard|       1| 79.99|2024-01-16|\n",
      "|       4|   Alice|   Monitor|       1|349.99|2024-01-17|\n",
      "|       5|   Diana|Headphones|       3| 59.99|2024-01-18|\n",
      "+--------+--------+----------+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INSERT — Add More Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------+-------+----------+\n",
      "|order_id|customer|   product|quantity|  price|order_date|\n",
      "+--------+--------+----------+--------+-------+----------+\n",
      "|       1|   Alice|    Laptop|       1| 999.99|2024-01-15|\n",
      "|       2|     Bob|     Mouse|       2|  29.99|2024-01-16|\n",
      "|       3| Charlie|  Keyboard|       1|  79.99|2024-01-16|\n",
      "|       4|   Alice|   Monitor|       1| 349.99|2024-01-17|\n",
      "|       5|   Diana|Headphones|       3|  59.99|2024-01-18|\n",
      "|       6|     Eve|    Webcam|       1|  89.99|2024-01-19|\n",
      "|       7|     Bob|   USB Hub|       2|  24.99|2024-01-20|\n",
      "|       8| Charlie|    Laptop|       1|1099.99|2024-02-01|\n",
      "+--------+--------+----------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    INSERT INTO demo.ecommerce.orders VALUES\n",
    "        (6,  'Eve',   'Webcam',   1, 89.99,  DATE '2024-01-19'),\n",
    "        (7,  'Bob',   'USB Hub',  2, 24.99,  DATE '2024-01-20'),\n",
    "        (8,  'Charlie', 'Laptop', 1, 1099.99, DATE '2024-02-01')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. UPDATE — Fix a Price\n",
    "\n",
    "Oops — Alice's laptop was supposed to be $899.99, not $999.99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------+--------+------+----------+\n",
      "|order_id|customer|product|quantity| price|order_date|\n",
      "+--------+--------+-------+--------+------+----------+\n",
      "|       1|   Alice| Laptop|       1|899.99|2024-01-15|\n",
      "+--------+--------+-------+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    UPDATE demo.ecommerce.orders\n",
    "    SET price = 899.99\n",
    "    WHERE order_id = 1\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders WHERE order_id = 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DELETE — Remove a Cancelled Order\n",
    "\n",
    "Eve cancelled her webcam order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------+-------+----------+\n",
      "|order_id|customer|   product|quantity|  price|order_date|\n",
      "+--------+--------+----------+--------+-------+----------+\n",
      "|       1|   Alice|    Laptop|       1| 899.99|2024-01-15|\n",
      "|       2|     Bob|     Mouse|       2|  29.99|2024-01-16|\n",
      "|       3| Charlie|  Keyboard|       1|  79.99|2024-01-16|\n",
      "|       4|   Alice|   Monitor|       1| 349.99|2024-01-17|\n",
      "|       5|   Diana|Headphones|       3|  59.99|2024-01-18|\n",
      "|       7|     Bob|   USB Hub|       2|  24.99|2024-01-20|\n",
      "|       8| Charlie|    Laptop|       1|1099.99|2024-02-01|\n",
      "+--------+--------+----------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    DELETE FROM demo.ecommerce.orders\n",
    "    WHERE order_id = 6\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MERGE INTO — Upsert New Data\n",
    "\n",
    "`MERGE INTO` lets you do an \"upsert\": update rows that match, insert rows that don't.\n",
    "\n",
    "This is one of Iceberg's most powerful features for incremental data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------+-------+----------+\n",
      "|order_id|customer|   product|quantity|  price|order_date|\n",
      "+--------+--------+----------+--------+-------+----------+\n",
      "|       1|   Alice|    Laptop|       1| 899.99|2024-01-15|\n",
      "|       2|     Bob|     Mouse|       2|  24.99|2024-01-16|\n",
      "|       3| Charlie|  Keyboard|       1|  79.99|2024-01-16|\n",
      "|       4|   Alice|   Monitor|       1| 349.99|2024-01-17|\n",
      "|       5|   Diana|Headphones|       3|  59.99|2024-01-18|\n",
      "|       7|     Bob|   USB Hub|       2|  24.99|2024-01-20|\n",
      "|       8| Charlie|    Laptop|       1|1099.99|2024-02-01|\n",
      "|       9|   Diana|     Mouse|       1|  29.99|2024-02-02|\n",
      "|      10|   Frank|  Keyboard|       2|  79.99|2024-02-03|\n",
      "+--------+--------+----------+--------+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view with incoming data\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW incoming_orders AS\n",
    "    SELECT * FROM VALUES\n",
    "        (2,  'Bob',   'Mouse',      2,  24.99,  DATE '2024-01-16'),  -- updated price\n",
    "        (9,  'Diana', 'Mouse',      1,  29.99,  DATE '2024-02-02'),  -- new order\n",
    "        (10, 'Frank', 'Keyboard',   2,  79.99,  DATE '2024-02-03')   -- new order\n",
    "    AS t(order_id, customer, product, quantity, price, order_date)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    MERGE INTO demo.ecommerce.orders target\n",
    "    USING incoming_orders source\n",
    "    ON target.order_id = source.order_id\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaway\n",
    "\n",
    "| Operation  | Traditional Hive           | Iceberg                    |\n",
    "|------------|----------------------------|----------------------------|\n",
    "| INSERT     | Supported                  | Supported                  |\n",
    "| UPDATE     | Rewrite entire partition   | Row-level update           |\n",
    "| DELETE     | Rewrite entire partition   | Row-level delete           |\n",
    "| MERGE INTO | Not natively supported     | Built-in upsert support    |\n",
    "\n",
    "Every operation we just ran created a **new snapshot** — we'll explore those in the next notebook!\n",
    "\n",
    "**Next up:** Time travel in notebook 03!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
