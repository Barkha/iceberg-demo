{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 — Partitioning\n",
    "\n",
    "Iceberg introduces **hidden partitioning** — users don't need to know how a table is partitioned to query it correctly. Iceberg also supports **partition evolution**, letting you change the partitioning scheme without rewriting data.\n",
    "\n",
    "In this notebook:\n",
    "1. Create a partitioned table\n",
    "2. See hidden partitioning in action\n",
    "3. Evolve the partition scheme\n",
    "4. Demonstrate partition pruning with EXPLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IcebergDemo\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\")\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.demo.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"../warehouse\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"Spark + Iceberg ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a Partitioned Table\n",
    "\n",
    "In Hive, you'd partition by a literal column (e.g., `order_date`), and users would have to include that column in queries.\n",
    "\n",
    "Iceberg uses **partition transforms** — you can partition by `month(order_date)` and Iceberg handles it transparently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS demo.ecommerce.orders_partitioned\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE demo.ecommerce.orders_partitioned (\n",
    "        order_id    INT,\n",
    "        customer    STRING,\n",
    "        product     STRING,\n",
    "        quantity    INT,\n",
    "        price       DOUBLE,\n",
    "        order_date  DATE\n",
    "    )\n",
    "    USING iceberg\n",
    "    PARTITIONED BY (month(order_date))\n",
    "\"\"\")\n",
    "\n",
    "print(\"Partitioned table created (partitioned by month of order_date).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert data spanning several months\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO demo.ecommerce.orders_partitioned VALUES\n",
    "        (1,  'Alice',   'Laptop',     1, 999.99,  DATE '2024-01-15'),\n",
    "        (2,  'Bob',     'Mouse',      2, 29.99,   DATE '2024-01-16'),\n",
    "        (3,  'Charlie', 'Keyboard',   1, 79.99,   DATE '2024-02-01'),\n",
    "        (4,  'Diana',   'Monitor',    1, 349.99,  DATE '2024-02-14'),\n",
    "        (5,  'Eve',     'Headphones', 3, 59.99,   DATE '2024-03-01'),\n",
    "        (6,  'Frank',   'Webcam',     1, 89.99,   DATE '2024-03-15'),\n",
    "        (7,  'Alice',   'Tablet',     1, 449.99,  DATE '2024-04-01'),\n",
    "        (8,  'Bob',     'Charger',    2, 19.99,   DATE '2024-04-10')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders_partitioned ORDER BY order_date\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hidden Partitioning in Action\n",
    "\n",
    "Users query by `order_date` — they don't need to know it's partitioned by month. Iceberg automatically prunes partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This query only needs to read the January partition\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM demo.ecommerce.orders_partitioned\n",
    "    WHERE order_date >= DATE '2024-01-01' AND order_date < DATE '2024-02-01'\n",
    "    ORDER BY order_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. View Partition Info\n",
    "\n",
    "Iceberg metadata tables show partition details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT partition, record_count, file_count\n",
    "    FROM demo.ecommerce.orders_partitioned.partitions\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Partition Pruning with EXPLAIN\n",
    "\n",
    "Let's prove that Iceberg prunes partitions by looking at the query plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    EXPLAIN\n",
    "    SELECT * FROM demo.ecommerce.orders_partitioned\n",
    "    WHERE order_date = DATE '2024-03-01'\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Partition Evolution\n",
    "\n",
    "Business changed — now you want to partition by **day** instead of month.\n",
    "\n",
    "In Hive, this would require recreating the table and rewriting all data. In Iceberg, it's a metadata-only operation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change partitioning from month to day — no data rewrite!\n",
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.ecommerce.orders_partitioned\n",
    "    REPLACE PARTITION FIELD month(order_date) WITH day(order_date)\n",
    "\"\"\")\n",
    "\n",
    "print(\"Partition scheme evolved from month(order_date) to day(order_date).\")\n",
    "print(\"Old data files are unchanged — only new writes use the new scheme.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert new data — will use the new day-level partitioning\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO demo.ecommerce.orders_partitioned VALUES\n",
    "        (9,  'Charlie', 'SSD',    1, 129.99, DATE '2024-05-01'),\n",
    "        (10, 'Diana',   'RAM',    2, 64.99,  DATE '2024-05-02')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    SELECT partition, record_count, file_count\n",
    "    FROM demo.ecommerce.orders_partitioned.partitions\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All data is still queryable seamlessly\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM demo.ecommerce.orders_partitioned\n",
    "    ORDER BY order_date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaway\n",
    "\n",
    "| Feature                | Hive                              | Iceberg                          |\n",
    "|------------------------|-----------------------------------|----------------------------------|\n",
    "| Partitioning           | Explicit partition columns        | Hidden partition transforms      |\n",
    "| User must know layout? | Yes — queries need partition cols  | No — Iceberg handles it          |\n",
    "| Change partitioning    | Recreate table + rewrite data     | Metadata-only, instant           |\n",
    "| Mixed partition layouts | Not possible                     | Old + new layouts coexist        |\n",
    "\n",
    "---\n",
    "\n",
    "## That's a Wrap!\n",
    "\n",
    "You've seen the core features of Apache Iceberg:\n",
    "1. **Table creation** with a standard SQL interface\n",
    "2. **Full CRUD** with row-level updates\n",
    "3. **Time travel** and snapshot management\n",
    "4. **Schema evolution** without data rewrites\n",
    "5. **Hidden partitioning** with partition evolution\n",
    "\n",
    "All of this runs locally with zero infrastructure — just Spark + Iceberg."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
