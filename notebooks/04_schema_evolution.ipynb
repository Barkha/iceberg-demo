{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 — Schema Evolution\n",
    "\n",
    "Iceberg tracks schema by column **ID** (not by name or position), so you can safely:\n",
    "- Add new columns\n",
    "- Drop columns\n",
    "- Rename columns\n",
    "- Widen types (e.g., INT → BIGINT)\n",
    "\n",
    "Old data files are **never rewritten** — Iceberg handles the mapping at read time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/23 13:57:36 WARN Utils: Your hostname, barkha-xg1 resolves to a loopback address: 127.0.1.1; using 192.168.1.227 instead (on interface enp195s0)\n",
      "26/02/23 13:57:36 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /home/barkha/.ivy2/cache\n",
      "The jars for the packages stored in: /home/barkha/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-c29059ac-4652-4bca-a5f2-d08550546b8f;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 in central\n",
      ":: resolution report :: resolve 54ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.7.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-c29059ac-4652-4bca-a5f2-d08550546b8f\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/2ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/barkha/iceberg-demo/.venv/lib/python3.13/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/23 13:57:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/02/23 13:57:37 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "26/02/23 13:57:37 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "26/02/23 13:57:37 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark + Iceberg ready.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IcebergDemo\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\")\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.demo.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"../warehouse\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"Spark + Iceberg ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|col_name  |data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|order_id  |int      |NULL   |\n",
      "|customer  |string   |NULL   |\n",
      "|product   |string   |NULL   |\n",
      "|quantity  |int      |NULL   |\n",
      "|price     |double   |NULL   |\n",
      "|order_date|date     |NULL   |\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE demo.ecommerce.orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Add a Column\n",
    "\n",
    "Let's add a `status` column to track order status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------+\n",
      "|col_name  |data_type|comment|\n",
      "+----------+---------+-------+\n",
      "|order_id  |int      |NULL   |\n",
      "|customer  |string   |NULL   |\n",
      "|product   |string   |NULL   |\n",
      "|quantity  |int      |NULL   |\n",
      "|price     |double   |NULL   |\n",
      "|order_date|date     |NULL   |\n",
      "|status    |string   |NULL   |\n",
      "+----------+---------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/02/23 13:57:52 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.ecommerce.orders\n",
    "    ADD COLUMNS (status STRING)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DESCRIBE demo.ecommerce.orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------+------+----------+------+\n",
      "|order_id|customer|   product|quantity| price|order_date|status|\n",
      "+--------+--------+----------+--------+------+----------+------+\n",
      "|       1|   Alice|    Laptop|       1|999.99|2024-01-15|  NULL|\n",
      "|       2|     Bob|     Mouse|       2| 29.99|2024-01-16|  NULL|\n",
      "|       3| Charlie|  Keyboard|       1| 79.99|2024-01-16|  NULL|\n",
      "|       4|   Alice|   Monitor|       1|349.99|2024-01-17|  NULL|\n",
      "|       5|   Diana|Headphones|       3| 59.99|2024-01-18|  NULL|\n",
      "+--------+--------+----------+--------+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Existing rows show NULL for the new column — no data rewrite needed!\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+----------+--------+------+----------+-------+\n",
      "|order_id|customer|   product|quantity| price|order_date| status|\n",
      "+--------+--------+----------+--------+------+----------+-------+\n",
      "|       1|   Alice|    Laptop|       1|999.99|2024-01-15|shipped|\n",
      "|       2|     Bob|     Mouse|       2| 29.99|2024-01-16|shipped|\n",
      "|       3| Charlie|  Keyboard|       1| 79.99|2024-01-16|shipped|\n",
      "|       4|   Alice|   Monitor|       1|349.99|2024-01-17|shipped|\n",
      "|       5|   Diana|Headphones|       3| 59.99|2024-01-18|shipped|\n",
      "+--------+--------+----------+--------+------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set status for existing orders\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE demo.ecommerce.orders\n",
    "    SET status = 'shipped'\n",
    "    WHERE status IS NULL\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rename a Column\n",
    "\n",
    "Rename `customer` to `customer_name` — because Iceberg tracks columns by ID,\n",
    "this is safe and doesn't break existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+\n",
      "|order_id|customer_name|   product|\n",
      "+--------+-------------+----------+\n",
      "|       1|        Alice|    Laptop|\n",
      "|       2|          Bob|     Mouse|\n",
      "|       3|      Charlie|  Keyboard|\n",
      "|       4|        Alice|   Monitor|\n",
      "|       5|        Diana|Headphones|\n",
      "+--------+-------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.ecommerce.orders\n",
    "    RENAME COLUMN customer TO customer_name\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT order_id, customer_name, product FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Another Column with a Default Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------+-------------------------+\n",
      "|col_name        |data_type|comment                  |\n",
      "+----------------+---------+-------------------------+\n",
      "|order_id        |int      |NULL                     |\n",
      "|customer_name   |string   |NULL                     |\n",
      "|product         |string   |NULL                     |\n",
      "|quantity        |int      |NULL                     |\n",
      "|price           |double   |NULL                     |\n",
      "|order_date      |date     |NULL                     |\n",
      "|status          |string   |NULL                     |\n",
      "|shipping_address|string   |Customer shipping address|\n",
      "+----------------+---------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.ecommerce.orders\n",
    "    ADD COLUMNS (shipping_address STRING COMMENT 'Customer shipping address')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DESCRIBE demo.ecommerce.orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Drop a Column\n",
    "\n",
    "We don't need `shipping_address` after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------+-------+\n",
      "|col_name     |data_type|comment|\n",
      "+-------------+---------+-------+\n",
      "|order_id     |int      |NULL   |\n",
      "|customer_name|string   |NULL   |\n",
      "|product      |string   |NULL   |\n",
      "|quantity     |int      |NULL   |\n",
      "|price        |double   |NULL   |\n",
      "|order_date   |date     |NULL   |\n",
      "|status       |string   |NULL   |\n",
      "+-------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.ecommerce.orders\n",
    "    DROP COLUMN shipping_address\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DESCRIBE demo.ecommerce.orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Old Data Still Readable\n",
    "\n",
    "Even after all these schema changes, we can still time-travel back to snapshots with the old schema. Iceberg reconciles the schema differences at read time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from earliest snapshot (8255495233418216272) with the CURRENT schema:\n",
      "+--------+--------+----------+--------+------+----------+\n",
      "|order_id|customer|   product|quantity| price|order_date|\n",
      "+--------+--------+----------+--------+------+----------+\n",
      "|       1|   Alice|    Laptop|       1|999.99|2024-01-15|\n",
      "|       2|     Bob|     Mouse|       2| 29.99|2024-01-16|\n",
      "|       3| Charlie|  Keyboard|       1| 79.99|2024-01-16|\n",
      "|       4|   Alice|   Monitor|       1|349.99|2024-01-17|\n",
      "|       5|   Diana|Headphones|       3| 59.99|2024-01-18|\n",
      "+--------+--------+----------+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the first snapshot (before schema changes)\n",
    "first_snapshot = spark.sql(\"\"\"\n",
    "    SELECT snapshot_id FROM demo.ecommerce.orders.snapshots\n",
    "    ORDER BY committed_at\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0].snapshot_id\n",
    "\n",
    "print(f\"Reading from earliest snapshot ({first_snapshot}) with the CURRENT schema:\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM demo.ecommerce.orders\n",
    "    VERSION AS OF {first_snapshot}\n",
    "    ORDER BY order_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaway\n",
    "\n",
    "| Operation           | Effect on existing data files |\n",
    "|---------------------|-------------------------------|\n",
    "| Add column          | None — new column reads as NULL |\n",
    "| Drop column         | None — column is hidden at read time |\n",
    "| Rename column       | None — mapped by column ID |\n",
    "| Widen type          | None — handled at read time |\n",
    "\n",
    "No data migration needed. No downtime. Schema changes are **metadata-only**.\n",
    "\n",
    "**Next up:** Partitioning in notebook 05!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
