{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 — Schema Evolution\n",
    "\n",
    "Iceberg tracks schema by column **ID** (not by name or position), so you can safely:\n",
    "- Add new columns\n",
    "- Drop columns\n",
    "- Rename columns\n",
    "- Widen types (e.g., INT → BIGINT)\n",
    "\n",
    "Old data files are **never rewritten** — Iceberg handles the mapping at read time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IcebergDemo\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.7.1\")\n",
    "    .config(\"spark.sql.catalog.demo\", \"org.apache.iceberg.spark.SparkCatalog\")\n",
    "    .config(\"spark.sql.catalog.demo.type\", \"hadoop\")\n",
    "    .config(\"spark.sql.catalog.demo.warehouse\", \"../warehouse\")\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"Spark + Iceberg ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Current Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DESCRIBE demo.ecommerce.orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Add a Column\n",
    "\n",
    "Let's add a `status` column to track order status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.ecommerce.orders\n",
    "    ADD COLUMNS (status STRING)\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DESCRIBE demo.ecommerce.orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Existing rows show NULL for the new column — no data rewrite needed!\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set status for existing orders\n",
    "spark.sql(\"\"\"\n",
    "    UPDATE demo.ecommerce.orders\n",
    "    SET status = 'shipped'\n",
    "    WHERE status IS NULL\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT * FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rename a Column\n",
    "\n",
    "Rename `customer` to `customer_name` — because Iceberg tracks columns by ID,\n",
    "this is safe and doesn't break existing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.ecommerce.orders\n",
    "    RENAME COLUMN customer TO customer_name\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"SELECT order_id, customer_name, product FROM demo.ecommerce.orders ORDER BY order_id\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add Another Column with a Default Comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.ecommerce.orders\n",
    "    ADD COLUMNS (shipping_address STRING COMMENT 'Customer shipping address')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DESCRIBE demo.ecommerce.orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Drop a Column\n",
    "\n",
    "We don't need `shipping_address` after all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    ALTER TABLE demo.ecommerce.orders\n",
    "    DROP COLUMN shipping_address\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"DESCRIBE demo.ecommerce.orders\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Old Data Still Readable\n",
    "\n",
    "Even after all these schema changes, we can still time-travel back to snapshots with the old schema. Iceberg reconciles the schema differences at read time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first snapshot (before schema changes)\n",
    "first_snapshot = spark.sql(\"\"\"\n",
    "    SELECT snapshot_id FROM demo.ecommerce.orders.snapshots\n",
    "    ORDER BY committed_at\n",
    "    LIMIT 1\n",
    "\"\"\").collect()[0].snapshot_id\n",
    "\n",
    "print(f\"Reading from earliest snapshot ({first_snapshot}) with the CURRENT schema:\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM demo.ecommerce.orders\n",
    "    VERSION AS OF {first_snapshot}\n",
    "    ORDER BY order_id\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaway\n",
    "\n",
    "| Operation           | Effect on existing data files |\n",
    "|---------------------|-------------------------------|\n",
    "| Add column          | None — new column reads as NULL |\n",
    "| Drop column         | None — column is hidden at read time |\n",
    "| Rename column       | None — mapped by column ID |\n",
    "| Widen type          | None — handled at read time |\n",
    "\n",
    "No data migration needed. No downtime. Schema changes are **metadata-only**.\n",
    "\n",
    "**Next up:** Partitioning in notebook 05!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
